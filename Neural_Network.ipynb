{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing the data\n",
    "final_df=pd.DataFrame()\n",
    "a_df=pd.DataFrame()\n",
    "b_df=pd.DataFrame()\n",
    "c_df=pd.DataFrame()\n",
    "\n",
    "for file_n in os.listdir('listings'):\n",
    "    if file_n[0]=='.':\n",
    "        continue\n",
    "    else:\n",
    "        add_df = pd.read_csv(f'listings/{file_n}')\n",
    "        if len(add_df.columns)==106:\n",
    "            final_df = pd.concat([final_df, add_df], axis=0)\n",
    "        elif len(add_df.columns)==96:\n",
    "            a_df = pd.concat([a_df, add_df], axis=0)\n",
    "        elif len(add_df.columns)==95:\n",
    "            b_df = pd.concat([b_df, add_df], axis=0)\n",
    "        else:\n",
    "            c_df = pd.concat([c_df, add_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#These are the columns not present in 2018 reports. They will be dropped.\n",
    "not_in_2018 = ['minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights',\n",
    "       'minimum_nights_avg_ntm','maximum_nights_avg_ntm', 'number_of_reviews_ltm', \n",
    "       'calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms', \n",
    "       'calculated_host_listings_count_shared_rooms']\n",
    "sf_df = final_df.drop(columns=not_in_2018)\n",
    "sf_df = pd.concat([sf_df, a_df], axis=0)\n",
    "\n",
    "#drop one column not in 2017\n",
    "sf_df = sf_df.drop(columns=['is_business_travel_ready'])\n",
    "sf_df = pd.concat([sf_df, b_df], axis=0)\n",
    "\n",
    "#drop 3 columns not in 2016 & Prior\n",
    "sf_df = sf_df.drop(columns=['access', 'interaction', 'house_rules'])\n",
    "sf_df = pd.concat([sf_df, c_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "#Converting Existing Data Columns to Dates\n",
    "sf_df['last_scraped'] = pd.to_datetime(sf_df['last_scraped'])\n",
    "sf_df['host_since'] = pd.to_datetime(sf_df['host_since'])\n",
    "#Adding new date features\n",
    "sf_df['year'] = sf_df['last_scraped'].transform(lambda x: x.year)\n",
    "sf_df['month-year'] = sf_df['last_scraped'].transform(lambda x: f'{x.month} {x.year}')\n",
    "sf_df['month'] = sf_df['last_scraped'].transform(lambda x: x.month)\n",
    "sf_df['day_of_week'] = sf_df['last_scraped'].transform(lambda x: x.dt.dayofweek)\n",
    "sf_df['day'] = sf_df['last_scraped'].transform(lambda x: x.day)\n",
    "\n",
    "#Converting All Price Related Columns from Objects to Floats\n",
    "sf_df['price'] = sf_df['price'].transform(lambda x: float(x.replace(',', '').replace('$', '')))\n",
    "sf_df['extra_people'] = sf_df['extra_people'].transform(lambda x: float(x.replace(',', '').replace('$', '')))\n",
    "\n",
    "#Fill NaNs in fee columns with 0's, because no additional fee, then convert\n",
    "sf_df[['security_deposit','cleaning_fee']] = sf_df[['security_deposit','cleaning_fee']].fillna(int(0))\n",
    "sf_df['security_deposit'] = sf_df['security_deposit'].transform(lambda x: 0 if x==int(0) else float(x.replace(',', '').replace('$', '')))\n",
    "sf_df['cleaning_fee'] = sf_df['cleaning_fee'].transform(lambda x: 0 if x==int(0) else float(x.replace(',', '').replace('$', '')))\n",
    "\n",
    "#Dropping columns that have over 75% null\n",
    "over_70_null = sf_df.columns[sf_df.isnull().sum()/len(sf_df) > 0.70]\n",
    "sf_df = sf_df.drop(columns=over_70_null)\n",
    "\n",
    "#Removing Outliers (0 & over 2000 daily rate)\n",
    "outlier_thresh=2000\n",
    "sf_df = sf_df.loc[~((sf_df['price'] == 0) | (sf_df['price'] > outlier_thresh))]\n",
    "\n",
    "#Filling the NaNs in beds, bathrooms, bedrooms\n",
    "#Assume that if the tenant has the full apartment, they have a bathroom/bedroom\n",
    "cond1= (sf_df['room_type']=='Entire home/apt')\n",
    "sf_df.loc[cond1 & (sf_df['beds']==0), sf_df.columns=='beds'] = 1\n",
    "sf_df.loc[cond1 & (sf_df['bathrooms']==0), sf_df.columns=='bathrooms'] = 1\n",
    "#For the rest, just fill na\n",
    "sf_df[['beds','bathrooms', 'bedrooms']] = sf_df[['beds','bathrooms', 'bedrooms']].fillna(0)\n",
    "\n",
    "#For review scores, let's just fillin with the average\n",
    "review_lst = ['review_scores_accuracy', 'review_scores_checkin',\n",
    "       'review_scores_cleanliness', 'review_scores_communication',\n",
    "       'review_scores_location', 'review_scores_rating', 'review_scores_value']\n",
    "for review in review_lst:\n",
    "    sf_df[review] = sf_df[review].fillna(sf_df[review].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2, test = train_test_split(sf_df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Users/sherryduong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding in all feature engineering for best performing model with Random Forest\n",
    "train_df2['amenities'] = train_df2['amenities'].fillna('0')\n",
    "train_df2['num_amenities'] = train_df2['amenities'].transform(lambda x: len(x.split(',')))\n",
    "train_df2[['space','summary','description','name']] = train_df2[['space','summary','description','name']].fillna(0)\n",
    "train_df2['len_space'] = train_df2['space'].transform(lambda x: 0 if x==0 else len(x))\n",
    "train_df2['len_summary'] = train_df2['summary'].transform(lambda x: 0 if x==0 else len(x))\n",
    "train_df2['len_description'] = train_df2['description'].transform(lambda x: 0 if x==0 else len(x))\n",
    "train_df2['len_name'] = train_df2['name'].transform(lambda x: 0 if x==0 else len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['accommodates','bathrooms', 'bed_type','bedrooms', 'beds','cleaning_fee',\n",
    "          'extra_people', 'num_amenities','neighbourhood_cleansed','property_type',\n",
    "          'review_scores_cleanliness','len_space', 'len_summary', 'len_description',\n",
    "          'len_name', 'review_scores_rating', 'room_type', 'security_deposit', 'month', \n",
    "          'year']\n",
    "X7 = train_df2[features]\n",
    "y7 = train_df2['price'].apply(np.log)\n",
    "\n",
    "X7 = pd.get_dummies(X7, columns=['bed_type','neighbourhood_cleansed','property_type',\n",
    "                           'room_type','month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X7, y7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.utils.normalize(X_train, axis=1).values\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 173524 samples, validate on 19281 samples\n",
      "Epoch 1/10\n",
      "173524/173524 [==============================] - 27s 153us/sample - loss: 0.9398 - MeanSquaredError: 0.9398 - val_loss: 0.3804 - val_MeanSquaredError: 0.3804\n",
      "Epoch 2/10\n",
      "173524/173524 [==============================] - 8s 46us/sample - loss: 0.3658 - MeanSquaredError: 0.3658 - val_loss: 0.3489 - val_MeanSquaredError: 0.3489\n",
      "Epoch 3/10\n",
      "173524/173524 [==============================] - 9s 52us/sample - loss: 0.3122 - MeanSquaredError: 0.3122 - val_loss: 0.2744 - val_MeanSquaredError: 0.2744\n",
      "Epoch 4/10\n",
      "173524/173524 [==============================] - 9s 53us/sample - loss: 0.2519 - MeanSquaredError: 0.2519 - val_loss: 0.2359 - val_MeanSquaredError: 0.2359\n",
      "Epoch 5/10\n",
      "173524/173524 [==============================] - 8s 45us/sample - loss: 0.2346 - MeanSquaredError: 0.2346 - val_loss: 0.2274 - val_MeanSquaredError: 0.2274\n",
      "Epoch 6/10\n",
      "173524/173524 [==============================] - 8s 47us/sample - loss: 0.2261 - MeanSquaredError: 0.2261 - val_loss: 0.2197 - val_MeanSquaredError: 0.2197\n",
      "Epoch 7/10\n",
      "173524/173524 [==============================] - 8s 44us/sample - loss: 0.2191 - MeanSquaredError: 0.2191 - val_loss: 0.2151 - val_MeanSquaredError: 0.2151\n",
      "Epoch 8/10\n",
      "173524/173524 [==============================] - 9s 53us/sample - loss: 0.2126 - MeanSquaredError: 0.2126 - val_loss: 0.2052 - val_MeanSquaredError: 0.2052\n",
      "Epoch 9/10\n",
      "173524/173524 [==============================] - 8s 47us/sample - loss: 0.2080 - MeanSquaredError: 0.2080 - val_loss: 0.2019 - val_MeanSquaredError: 0.2019\n",
      "Epoch 10/10\n",
      "173524/173524 [==============================] - 8s 45us/sample - loss: 0.2041 - MeanSquaredError: 0.2041 - val_loss: 0.1978 - val_MeanSquaredError: 0.1978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4ccb5150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Input layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "#Second layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer='adam',\n",
    "             metrics=['MeanSquaredError'])\n",
    "model.fit(X,y, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = tf.keras.utils.normalize(X_val, axis=1).values\n",
    "y_val = np.array(y_val)\n",
    "preds = model.predict(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5694033226163753 0.5781447619611689\n"
     ]
    }
   ],
   "source": [
    "rmse = np.exp(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "r2 = r2_score(y_val, preds)\n",
    "print(rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add More Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 173524 samples, validate on 19281 samples\n",
      "Epoch 1/10\n",
      "173524/173524 [==============================] - 9s 51us/sample - loss: 1.0905 - MeanSquaredError: 1.0905 - val_loss: 0.2340 - val_MeanSquaredError: 0.2340 - MeanSquaredError\n",
      "Epoch 2/10\n",
      "173524/173524 [==============================] - 8s 46us/sample - loss: 0.2337 - MeanSquaredError: 0.2337 - val_loss: 0.2160 - val_MeanSquaredError: 0.2160\n",
      "Epoch 3/10\n",
      "173524/173524 [==============================] - 8s 47us/sample - loss: 0.2176 - MeanSquaredError: 0.2176 - val_loss: 0.2005 - val_MeanSquaredError: 0.2005\n",
      "Epoch 4/10\n",
      "173524/173524 [==============================] - 8s 48us/sample - loss: 0.2091 - MeanSquaredError: 0.2091 - val_loss: 0.1947 - val_MeanSquaredError: 0.1947\n",
      "Epoch 5/10\n",
      "173524/173524 [==============================] - 8s 47us/sample - loss: 0.2049 - MeanSquaredError: 0.2049 - val_loss: 0.1951 - val_MeanSquaredError: 0.1951\n",
      "Epoch 6/10\n",
      "173524/173524 [==============================] - 8s 45us/sample - loss: 0.2022 - MeanSquaredError: 0.2022 - val_loss: 0.2231 - val_MeanSquaredError: 0.2231\n",
      "Epoch 7/10\n",
      "173524/173524 [==============================] - 8s 44us/sample - loss: 0.2006 - MeanSquaredError: 0.2006 - val_loss: 0.1906 - val_MeanSquaredError: 0.1906\n",
      "Epoch 8/10\n",
      "173524/173524 [==============================] - 8s 45us/sample - loss: 0.1994 - MeanSquaredError: 0.1994 - val_loss: 0.1885 - val_MeanSquaredError: 0.1885\n",
      "Epoch 9/10\n",
      "173524/173524 [==============================] - 8s 48us/sample - loss: 0.1995 - MeanSquaredError: 0.1995 - val_loss: 0.2517 - val_MeanSquaredError: 0.2517\n",
      "Epoch 10/10\n",
      "173524/173524 [==============================] - 8s 44us/sample - loss: 0.1981 - MeanSquaredError: 0.1981 - val_loss: 0.1873 - val_MeanSquaredError: 0.1873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5892c510>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "#Input layer\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation(\"relu\"))\n",
    "\n",
    "#Second layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "#Third layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "#Fourth layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "#Output layer\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "model1.compile(loss='mean_squared_error',\n",
    "             optimizer='adam',\n",
    "             metrics=['MeanSquaredError'])\n",
    "model1.fit(X,y, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5482691700609095 0.6031435729747893\n"
     ]
    }
   ],
   "source": [
    "preds = model1.predict(X_val)\n",
    "rmse = np.exp(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "r2 = r2_score(y_val, preds)\n",
    "print(rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
